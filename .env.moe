# Mixture-of-Experts (MoE) Configuration
# Environment variables for ProjectAlpha MoE system

# =============================================================================
# MoE System Configuration
# =============================================================================

# Maximum RAM allocation for experts (in GB)
MOE_MAX_RAM_GB=8.0

# Cache strategy for expert management
MOE_CACHE_STRATEGY=lru

# Maximum number of parallel experts
MOE_PARALLEL_EXPERTS=2

# Classification threshold for expert selection
MOE_CLASSIFICATION_THRESHOLD=0.7

# Enable fallback to general reasoning expert
MOE_FALLBACK_ENABLED=true

# Timeout for expert loading/inference (seconds)
MOE_EXPERT_TIMEOUT=30

# Enable asynchronous expert loading
MOE_LOAD_ASYNC=true

# =============================================================================
# Expert Model Paths Configuration
# =============================================================================

# Logic Domain Experts
LOGIC_HIGH_MODEL_PATH=models/logic_high.gguf
LOGIC_HIGH_SIZE_GB=2.1

LOGIC_CODE_MODEL_PATH=models/logic_code.gguf
LOGIC_CODE_SIZE_GB=1.8

LOGIC_PROOF_MODEL_PATH=models/logic_proof.gguf
LOGIC_PROOF_SIZE_GB=2.3

LOGIC_FALLBACK_MODEL_PATH=models/logic_fallback.gguf
LOGIC_FALLBACK_SIZE_GB=1.5

# Emotional Domain Experts
EMOTE_VALENCE_MODEL_PATH=models/emote_valence.gguf
EMOTE_VALENCE_SIZE_GB=1.7

EMOTE_AROUSAL_MODEL_PATH=models/emote_arousal.gguf
EMOTE_AROUSAL_SIZE_GB=1.6

# Creative Domain Experts
CREATIVE_METAPHOR_MODEL_PATH=models/creative_metaphor.gguf
CREATIVE_METAPHOR_SIZE_GB=2.0

CREATIVE_WRITE_MODEL_PATH=models/creative_write.gguf
CREATIVE_WRITE_SIZE_GB=2.2

# Planning Domain Experts
PLANNING_TEMPORAL_MODEL_PATH=models/planning_temporal.gguf
PLANNING_TEMPORAL_SIZE_GB=1.9

PLANNING_STRATEGIC_MODEL_PATH=models/planning_strategic.gguf
PLANNING_STRATEGIC_SIZE_GB=2.1

# Specialized Domain Experts
RITUAL_SYMBOLIC_MODEL_PATH=models/ritual_symbolic.gguf
RITUAL_SYMBOLIC_SIZE_GB=1.8

MEMORY_RECALL_MODEL_PATH=models/memory_recall.gguf
MEMORY_RECALL_SIZE_GB=2.0

# =============================================================================
# Standard Model Configuration (Fallbacks)
# =============================================================================

# Core conductor models
CONDUCTOR_MODEL=gpt-oss-20b
LOGIC_MODEL=deepseek-coder:1.3b
EMOTION_MODEL=mistral:7b
CREATIVE_MODEL=mixtral:8x7b

# Model backend preferences
MODEL_BACKEND=auto
# Options: ollama, llamacpp, mock, auto

# =============================================================================
# Mobile/Resource-Constrained Deployment
# =============================================================================

# Mobile-optimized settings
MOE_MOBILE_MODE=false
MOE_MOBILE_MAX_RAM_GB=2.0
MOE_MOBILE_MAX_EXPERTS=1

# Preload priority experts for faster response
MOE_PRELOAD_EXPERTS=logic_fallback,emote_valence

# =============================================================================
# Development and Testing
# =============================================================================

# Enable debug logging
MOE_DEBUG=false

# Use mock models for testing
MOE_USE_MOCK_MODELS=false

# Performance monitoring
MOE_ENABLE_STATS=true
MOE_STATS_FILE=moe_stats.json
